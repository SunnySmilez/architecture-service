# 日志服务

> 补充介绍

## 格式规范

> 为了方便对日志的查询及聚合, 对日志字段进行了统一. 另外为了方便日志的解析处理, 推荐日志使用**json**的格式进行记录和传输.

### 应用日志(PHP)

**日志字典**

| 字段            | 说明                |
| ------------- | ----------------- |
| date          | 日志记录时间            |
| x_rid         | 请求ID              |
| server_ip     | 服务器IP             |
| client_ip     | 客户端IP             |
| uri           | 访问路径              |
| params        | 请求参数              |
| exectime      | 执行时间(毫秒)          |
| succ          | 执行结果 succ/fail    |
| retcode       | 返回码  成功为`2000000` |
| retmsg        | 返回信息              |
| error_type    | 错误类型              |
| error_code    | 错误码               |
| error_message | 错误信息              |
| error_trace   | 错误栈信息             |

### Nginx日志

**配置**

1. 修改nginx的日志格式配置 **/etc/nginx/nginx.conf**

```
log_format json '{"date":"$time_iso8601",'
                '"x_rid":"$request_id",'
                '"client_ip":"$remote_addr",'
                '"server_ip":"$server_addr",'
                '"domain":"$host",'
                '"uri":"$uri",'
                '"size":$body_bytes_sent,'
                '"responsetime":$request_time,'
                '"upstreamtime":"$upstream_response_time",'
                '"upstreamhost":"$upstream_addr",'
                '"xff":"$http_x_forwarded_for",'
                '"referer":"$http_referer",'
                '"agent":"$http_user_agent",'
                '"status":$status}';
```

2. 修改日志输出配置 **/etc/nginx/vhost/demo.conf**

```
#访问日志
access_log syslog:server={日志机ip}:515,facility=local6,tag={app}_info,severity=info json;
#错误日志
error_log syslog:server={日志机ip}:515,facility=local6,tag={app}_error,severity=info;
```

**日志字典**

| 字段           | 说明                 |
| ------------ | ------------------ |
| date         | 日志记录时间             |
| x_rid        | 请求ID               |
| server_ip    | 服务器IP              |
| client_ip    | 客户端IP              |
| domain       | 主机域名               |
| uri          | 访问路径               |
| size         | 请求体大小(字节)          |
| responsetime | 请求响应时间(秒)          |
| upstreamtime | nginx向后端转发请求的时间    |
| upstreamhost | nginx服务器地址         |
| xff          | 客户端真实ip(使用代理时)     |
| referer      | 请求来源               |
| agent        | 用户访问代理(系统信息 浏览器信息) |
| status       | http 状态码           |

##  日志传输

> Syslog

### 安装

1. 安装 `yum install rsyslog` (大部分linux发行版已自带)

### 部署

1. 添加配置 `/etc/rsyslog.conf`

   ```yaml
   local7.*					@@[日志机ip]:514 #应用日志 
   local6.*					@@[日志机ip]:515 #nginx日志
   ```

2. 启动 `service rsyslog start`


## 日志队列

> Kafka

### 介绍

### 使用

## 日志解析

### Logstash安装部署

见[数据统计服务-数据采集](../stat/数据采集.md)

### 配置日志解析

1. 添加应用日志解析配置 **config/conf.d/app.conf**

```
   input {
           tcp {
               port => '514'
               mode => "server"
               add_field => ["log_type", "app"]
           }
   }                                
   filter {
     	if [log_type] == "app" {
       	   dissect {
               mapping => {
                   "message" => "%{?prefix} : [LOG_PATH:%{log_path}] %{log_content}"
               }
       	   }
       	   json {
               source => "log_content"
           }
           date {
               match => ["date", "yyyy-MM-dd HH:mm:ss"]
               timezone => "Asia/Shanghai"
           }
           mutate {
               remove_field => ["message", "host", "port", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9"] #去掉不必要的数据项
           }
           if [level] == "stat" {
          		grok {
                   match => { 
                       "log_path" => "(?<index>\w+)/\d+/\w+\.\d+\.log"
                   }
               	}
     	   }
     	}
   }
   output {
     if [log_type] == "app" {
       	if [level] == "stat" { #统计日志
       		elasticsearch {
              hosts => ["10.0.4.7:9200", "10.0.4.10:9200", "10.0.4.12:9200"]
              index => "stat-%{index}-%{+YYYYMM}"
              document_type => "%{app}"
              flush_size => 1000 #批量上送数据最大条数 不会超过pipeline.batch.size 默认500
              idle_flush_time => 1 #批量上送数据间隔 默认1s
       		}
       	}else{
          elasticsearch {
           	hosts => ["10.0.4.7:9200", "10.0.4.10:9200", "10.0.4.12:9200"]
           	index => "logstash-app-%{app}-%{+YYYYMMdd}"
           	document_type => "%{level}"
           	flush_size => 1000 #批量上送数据最大条数 不会超过pipeline.batch.size 默认500
           	idle_flush_time => 1 #批量上送数据间隔 默认1s
       	  }
          file {
               path => "/mnt/log2017/logs/app/%{app}/%{log_path}"
               gzip => false #按gzip方式压缩
               flush_interval => 2  #指定刷入间隔(秒数)，0代表实时写入 默认2s
               codec => line {
                      format => "%{log_content}"
              }
          }
       	}
     }
   }
```

2. 添加nginx日志解析配置 **config/conf.d/nginx.conf**

```
   input {
            syslog {
               port => '515'
               add_field => ["log_type", "nginx"]
            }
   }
   filter {
     	if [log_type] == "nginx" {
            dissect {
                mapping => {
                    "program" => "%{app}_%{level}"
                }
            }
           if [level] == "info" {
               json {
              		source => "message"
           	   }
             	dissect {
                   mapping => { 
                       "date" => "%{year}-%{month}-%{day}T%{hour}:%{minute}:%{second}+%{?timezone}"
                   }
               }
               mutate {
                   rename => ["message", "log_content"]
                   remove_field => ["date"]
               }
           }
           if [level] == "error" {
               grok {
                   match => { 
                       "message" => "(?<year>\d+)/(?<month>\d+)/(?<day>\d+)\s(?<hour>\d+):(?<minute>\d+):(?<second>\d+)\s"
                   }
               }
               mutate {
                   rename => ["message", "log_content"]
               }
           }

           mutate {
                   add_field => ["date", "%{year}-%{month}-%{day} %{hour}:%{minute}:%{second}"]
                   add_field => ["log_path", "%{year}%{month}/%{level}.%{year}%{month}%{day}.log"]
                   remove_field => ["message", "program", "host", "port", "year", "month", "day", "hour", "minute", "second"]
           }
           date {
               match => ["date", "yyyy-MM-dd HH:mm:ss"]
               timezone => "Asia/Shanghai"
           }
     	}   
   }
   output {
     	if [log_type] == "nginx" {
            elasticsearch {
                hosts => ["10.0.4.7:9200", "10.0.4.10:9200", "10.0.4.12:9200"]
                index => "logstash-nginx-%{app}-%{+YYYYMMdd}"
                document_type => "%{level}"
                flush_size => 1000 #批量上送数据最大条数 不会超过pipeline.batch.size 默认500
                idle_flush_time => 1 #批量上送数据间隔 默认1s
            }
            file {
                 path => "/mnt/log2017/logs/nginx/%{app}/%{log_path}"
                 gzip => false #按gzip方式压缩
                 flush_interval => 2  #指定刷入间隔(秒数)，0代表实时写入 默认2s
                 codec => line {
                        format => "%{log_content}"
                }
            }
     	}		
   }
```

3. 启动 

   ```shell
   bin/logstash -f config/conf.d -l /var/log/logstash 2>&1 &
   ```

   - -w  指定日志处理的线程数量  默认是 CPU 核数		

## 日志存储

### ES安装部署

见[数据统计服务-数据聚合](../stat/数据聚合.md)

### 日志存储调优

待补充 

## 服务报告

### Kibana安装部署

见[数据统计服务-数据可视化](../stat/数据可视化.md)

### 日志报表配置

1. 添加数据源
   - app `type:app AND level:"info" AND uri:API*`
   - nginx `type:nginx AND level:"info"`
2. 添加可视化
   - 访问量  `area chart`
   - 访问分布 `pie chart`
   - 平均执行时长 `line chart`
   - 成功率 `area chart `
3. 添加面板 组装可视化组件

## 日志查看

> 待补充

##  参考资料

> - [ELK](http://kibana.logstash.es/content/)
