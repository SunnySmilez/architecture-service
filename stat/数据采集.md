# 数据采集

> 采集数据支持结构化和非结构化的数据, 对数据统计的场景更友好, 能够支持各种数据源.

## 数据处理节点

数据采集节点负责从数据源处获取数据, 格式化处理后上送到数据仓库. 

而**Logstash** 是一款强大的数据处理工具，它可以实现数据传输，格式处理，格式化输出，还有强大的插件功能, 故我们选择**logstash**作为中间节点来收集和处理数据.

### 安装

1. 下载安装包 

   ```shell
   wget -O logstash.rpm https://artifacts.elastic.co/downloads/logstash/logstash-5.2.0.rpm
   ```

2. 安装 `yum install logstash.rpm` 

   - 工作目录: `/usr/share/logstash`

3. 安装插件 

   - [字符串解析插件](https://rubygems.org/downloads/logstash-filter-dissect-1.0.8.gem)  

     ```shell
     bin/logstash-plugin install logstash-filter-dissect
     ```

### 部署

1. 创建配置文件目录 

   ```shell
   mkdir -p /usr/share/logstash/config/
   ```

2. 复制默认配置到工作目录

   ```shell
   cp -r /etc/logstash/* /usr/share/logstash/config/
   ```

3. 添加配置 **config/logstash.yml**

   ```yaml
   pipeline.batch.size: 1000 #单线程处理前最大累积的日志条数 默认125
   pipeline.batch.delay: 5   #单线程在打包批量日志的时候最多等待几毫秒 默认5ms    
   ```

4. 启动

   ```shell
   bin/logstash -f config/conf.d -l /var/log/logstash 2>&1 &
   ```

   - -w  配置日志处理的线程数量  默认是 CPU 核数

## 数据采集方案

### 打点

> 支持在应用中直接记录需要统计的数据项. 将没有落地的数据纳入统计

数据打点通过记录日志的方式, 日志级别统一为`stat`. 数据将会以`syslog`的方式送到数据处理节点`logstash`, 在节点处格式化数据之后, 再上送到数据仓库中进行存储. 为了减轻数据节点的格式化负担, 推荐统计数据以**JSON**的形式送入.

### DB

> 应用数据以各种形式存储在多种数据库中, 如`mysql` `redis`等. 当我们需要依托应用数据进行数据统计时, 就需要将数据同步到数据仓库中.

#### mysql

 [**logstash-input-jdbc**](https://github.com/logstash-plugins/logstash-input-jdbc) 

> logstash插件 官方推荐

**使用示例**(待补充)

[**go-mysql-elasticsearch**](https://github.com/siddontang/go-mysql-elasticsearch) 

> GitHub上的开源工具, 支持数据的全量同步及增量同步.  其具体介绍及使用方法可见项目页.

**使用示例**(待补充)

### 本地文件

> 在业务进行过程中, 部分数据可能以本地文件的形式存在, 如`csv` `excel`等.

通过数据处理脚本解析数据后送入logstash

## 数据上送规范

待补充